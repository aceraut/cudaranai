#include "common.cuh"
#include "conv.cuh"

#include <algorithm>
#include <utility>
#include <vector>

#include <cuda_runtime.h>
#include <thrust/device_vector.h>

namespace nnv2 {

// In the forward phase of the convolution layer, the input consists of F_in
// feature maps, each with dimensions [in_h, in_w]. The output feature maps are
// generated by applying a convolution operation to small local patches of the
// input feature maps using a corresponding set of weights, known as filters.
// A unique bias value is then added to each pixel in its respective output
// feature map, resulting in F_out output feature maps.
//
// In this implementation, we avoid the traditional "sliding windows" approach,
// where we loop over all local patches of the input feature maps and further
// loop over all pixels of a patch to perform the convolution. Instead, the
// convolution operation is formulated as one large matrix multiplication, or
// matmul. The key idea is to transform the input feature maps so that all
// elements needed to compute a single output feature map pixel are stored as
// one sequential column in a matrix.
//
// Specifically, each input feature map is transformed into a representation
// where each row corresponds to a pixel in the filter, and each column
// corresponds to a pixel in the output feature map. Therefore, an input
// consisting of F_in feature maps is transformed into an array with dimensions
// [F_in, filter_h * filter_w, out_h * out_w]. We then perform matmul between
// the filter (reshaped to [F_out, F_in * filter_h * filter_w]) and the unrolled
// input (reshaped to [F_in * filter_h * filter_w, out_h * out_w]). Since the
// output has the shape [F_out, out_h * out_w] after the operation, it's
// reshaped to [F_out, out_h, out_w]. The same also applies to batch operation.

// Unrolls the input feature maps
__global__ void im2col_kernel(int size, const float *im, float *col, int in_h,
                              int in_w, int pad_h, int pad_w, int filter_h,
                              int filter_w, int stride_h, int stride_w,
                              int out_h, int out_w, int im_stride,
                              int col_stride) {
    // Each thread handles a flattened column of size filter_h * filter_w.
    CUDA_GRID_STRIDE_LOOP(idx, size) {
        // Calculate base coords of top-left element of image on a filter.
        int out_x = (idx / out_w) % out_h;
        int out_y = idx % out_w;
        int in_x_start = out_x * stride_h - pad_h;
        int in_y_start = out_y * stride_w - pad_w;

        // Point to image and filter column that this thread handles.
        int feat_idx = blockIdx.y;
        im += feat_idx * im_stride;
        col += feat_idx * col_stride;

        for (int i = 0; i < filter_h; i++) {
            for (int j = 0; j < filter_w; j++) {
                int in_x = in_x_start + i;
                int in_y = in_y_start + j;

                if (in_x >= 0 && in_x < in_h && in_y >= 0 && in_y < in_w) {
                    col[idx] = im[in_x * in_h + in_y];
                } else {
                    col[idx] = 0;
                }

                col += size; // Unrolled image width is out_h * out_w = size
            }
        }
    }
}

void im2col(const Array *im, Array *col, int pad_h, int pad_w, int filter_h,
            int filter_w, int stride_h, int stride_w) {
    const ShapeType &im_shape = im->get_shape();

    int batch_size = im_shape[0];
    int im_feats = im_shape[1];
    int im_h = im_shape[2];
    int im_w = im_shape[3];

    // Launch kernels
    int out_h = (im_h + 2 * pad_h - filter_h) / stride_h + 1;
    int out_w = (im_w + 2 * pad_w - filter_w) / stride_w + 1;
    int im_stride = im_h * im_w;
    int col_stride = filter_h * filter_w * out_h * out_w;

    int size = out_h * out_w;
    dim3 grid_dim(utils::div_ceil(size, BLOCK_SIZE), batch_size * im_feats);

    const float *im_raw = RAW_PTR(im->get_vec());
    float *col_raw = RAW_PTR(col->get_vec());

    im2col_kernel<<<grid_dim, BLOCK_SIZE>>>(
        size, im_raw, col_raw, im_h, im_w, pad_h, pad_w, filter_h, filter_w,
        stride_h, stride_w, out_h, out_w, im_stride, col_stride);
    CUDA_POST_KERNEL_CHECK;
}

void conv_forward(Array *output, const Array *input, Array *col, Array *filter,
                  int pad_h, int pad_w, int stride_h, int stride_w) {
    const ShapeType &input_shape = input->get_shape();
    const ShapeType &output_shape = output->get_shape();
    const ShapeType &col_shape = col->get_shape();
    const ShapeType &filter_shape = filter->get_shape();

    CHECK_EQ(input_shape.size(), 4, "conv_forward: input shape error");
    CHECK_EQ(output_shape.size(), 4, "conv_forward: output shape error");
    CHECK_EQ(col_shape.size(), 3, "conv_forward: col shape error");
    CHECK_EQ(filter_shape.size(), 4, "conv_forward: filter shape error");

    int batch_size = input_shape[0];
    int in_feats = input_shape[1];

    CHECK_EQ(output_shape[0], batch_size, "conv_forward: batch size error");

    int out_feats = output_shape[1];
    int out_h = output_shape[2];
    int out_w = output_shape[3];

    CHECK_EQ(filter_shape[0], out_feats, "conv_forward: feature size error");
    CHECK_EQ(filter_shape[1], in_feats, "conv_forward: feature size error");

    int filter_h = filter_shape[2];
    int filter_w = filter_shape[3];

    // Cols = im2col(X)
    im2col(input, col, pad_h, pad_w, filter_h, filter_w, stride_w, stride_h);

    // Reshape K to (o_f, i_f * k_h * k_w)
    filter->reshape({out_feats, in_feats * filter_h * filter_w});
    // Reshape Y to (n, o_f, o_h * o_w)
    output->reshape({batch_size, out_feats, out_h * out_w});

    // Y = K * Cols
    ops::matmul(output, filter, col, 1);

    // Recover shape
    filter->reshape({out_feats, in_feats, filter_h, filter_w});
    output->reshape({batch_size, out_feats, out_h, out_w});
}

__global__ void conv_forward_bias_kernel(int size, float *output,
                                         const float *bias, int im_stride,
                                         int out_feats) {
    CUDA_GRID_STRIDE_LOOP(idx, size) {
        int bias_idx = (idx / im_stride) % out_feats;
        output[idx] += bias[bias_idx];
    }
}

void conv_forward_bias(Array *output, const Array *bias) {
    const ShapeType &output_shape = output->get_shape();
    const ShapeType &bias_shape = bias->get_shape();

    int batch_size = output_shape[0];
    int out_feats = output_shape[1];
    int out_h = output_shape[2];
    int out_w = output_shape[3];

    CHECK_EQ(bias_shape[0], 1, "conv_forward_bias: bias isn't a column vector");
    CHECK_EQ(bias_shape[1], out_feats,
             "conv_forward_bias: mismatch between bias size and number of "
             "output features");

    int size = output->get_vec().size();
    int grid_size = utils::div_ceil(size, BLOCK_SIZE);
    float *output_raw = RAW_PTR(output->get_vec());
    const float *bias_raw = RAW_PTR(bias->get_vec());

    conv_forward_bias_kernel<<<grid_size, BLOCK_SIZE>>>(
        size, output_raw, bias_raw, out_h * out_w, out_feats);
    CUDA_POST_KERNEL_CHECK;
}

// The backward phase of the convolution layer involves calculating the loss
// gradients with respect to the input, filter, and bias.
// Given that the formula for the forward phase is: Y = F * Cols + b, where F is
// the filter, Cols is the unrolled input, and b is the bias, the formulas for
// the backward phase are as follows:
//
// - Loss gradient w.r.t filter:  dL/dF = dL/dY * Cols^T
// - Loss gradient w.r.t unrolled input: dL/dCols = F^T * dL/dY
// - Loss gradient w.r.t bias: dL/db = sum(dL/dY) along all dimensions except
//                                     the output features
//
// The loss gradient w.r.t input is obtained by reshaping the unrolled input
// gradient back to its original dimensions ([F_in, in_h, in_w]).

// Transforms unrolled input gradient to its original representation
// Note that unlike im2col, elements that have the same index in the original
// format are rolled back using sum and not assignment.
__global__ void col2im_kernel(int size, float *im, const float *col, int in_h,
                              int in_w, int pad_h, int pad_w, int filter_h,
                              int filter_w, int stride_h, int stride_w,
                              int out_h, int out_w, int im_stride,
                              int col_stride) {
    // Each thread handles a pixel in the input image
    CUDA_GRID_STRIDE_LOOP(idx, size) {
        // Point to image and filter column that this thread handles
        int feat_idx = blockIdx.y;
        im += feat_idx * im_stride;
        col += feat_idx * col_stride;

        // Calculate coord in input image
        int in_x = (idx / in_w) % in_h + pad_h;
        int in_y = idx % in_w + pad_w;

        // Locate the base coords of the section in the output image that
        // pixel at coord (r, c) in the input image is involved
        int out_x_start =
            (in_x < filter_h) ? 0 : (in_x - filter_h) / stride_h + 1;
        int out_x_end = fminf(out_h, in_x / stride_h + 1);
        int out_y_start =
            (in_y < filter_w) ? 0 : (in_y - filter_w) / stride_w + 1;
        int out_y_end = fminf(out_w, in_y / stride_w + 1);

        float value = 0;
        for (int out_x = out_x_start; out_x < out_x_end; out_x++) {
            for (int out_y = out_y_start; out_y < out_y_end; out_y++) {
                // Locate the filter position that overlaps this pixel
                int filter_x = in_x - stride_h * out_x;
                int filter_y = in_y - stride_w * out_y;

                // Find index in column and perform sum
                int col_idx =
                    ((filter_x * filter_w + filter_y) * out_h + out_x) * out_w +
                    out_y;
                value += col[col_idx];
            }
        }
        im[idx] = value;
    }
}

void col2im(const Array *col, Array *im, int pad_h, int pad_w, int filter_h,
            int filter_w, int stride_h, int stride_w) {
    const ShapeType &im_shape = im->get_shape();

    int batch_size = im_shape[0];
    int im_feats = im_shape[1];
    int im_h = im_shape[2];
    int im_w = im_shape[3];

    // Launch kernels
    int out_h = (im_h + 2 * pad_h - filter_h) / stride_h + 1;
    int out_w = (im_w + 2 * pad_w - filter_w) / stride_w + 1;
    int im_stride = im_h * im_w;
    int col_stride = filter_h * filter_w * out_h * out_w;

    int size = im_h * im_w;
    dim3 grid_dim(utils::div_ceil(size, BLOCK_SIZE), batch_size * im_feats);

    float *im_raw = RAW_PTR(im->get_vec());
    const float *col_raw = RAW_PTR(col->get_vec());

    col2im_kernel<<<grid_dim, BLOCK_SIZE>>>(
        size, im_raw, col_raw, im_h, im_w, pad_h, pad_w, filter_h, filter_w,
        stride_h, stride_w, out_h, out_w, im_stride, col_stride);
    CUDA_POST_KERNEL_CHECK;
}

// Calculates the input gradient, filter gradient from the output gradient
void conv_backward(Array *input_grad, Array *filter_grad, Array *output_grad,
                   const Array *input, Array *filter, const Array *col,
                   int pad_h, int pad_w, int stride_h, int stride_w,
                   ArrayMap &cache) {
    const ShapeType &input_grad_shape = input_grad->get_shape();
    const ShapeType &filter_grad_shape = filter_grad->get_shape();
    const ShapeType &output_grad_shape = output_grad->get_shape();
    const ShapeType &input_shape = input->get_shape();
    const ShapeType &filter_shape = filter->get_shape();
    const ShapeType &col_shape = col->get_shape();

    CHECK_EQ(input_grad_shape.size(), 4,
             "conv_backward: input gradient shape error");
    CHECK_EQ(filter_grad_shape.size(), 4, "conv_backward: input shape error");
    CHECK_EQ(col_shape.size(), 3, "conv_backward: col shape error");
    CHECK_EQ(output_grad_shape.size(), 4,
             "conv_backward: output gradient shape error");

    CHECK_EQ(input_shape, input_grad_shape,
             "conv_backward: shape mismatch between input and its gradient");
    CHECK_EQ(filter_shape, filter_grad_shape,
             "conv_backward: shape mismatch between filter and its gradient");

    int batch_size = input_shape[0];
    int in_feats = input_shape[1];

    CHECK_EQ(output_grad_shape[0], batch_size,
             "conv_backward: batch size error");

    int out_feats = output_grad_shape[1];
    int out_h = output_grad_shape[2];
    int out_w = output_grad_shape[3];

    CHECK_EQ(filter_shape[0], out_feats, "conv_backward: feature size error");
    CHECK_EQ(filter_shape[1], in_feats, "conv_backward: feature size error");

    int filter_h = filter_shape[2];
    int filter_w = filter_shape[3];

    // Reshape dL/dY to (n, o_f, o_h * o_w)
    output_grad->reshape({batch_size, out_feats, out_h * out_w});

    // Cols^T
    utils::set_array_cache(
        cache, "col_t",
        {batch_size, out_h * out_w, in_feats * filter_h * filter_w});
    ops::transpose(cache["col_t"].get(), col);

    // dL/dY * Cols^T
    utils::set_array_cache(
        cache, "filter_grad_unfolded",
        {batch_size, out_feats, in_feats * filter_h * filter_w});
    ops::matmul(cache["filter_grad_unfolded"].get(), output_grad,
                cache["col_t"].get());

    // dL/dK = sum(dL/dY * Cols^T) along the batch
    cache["filter_grad_unfolded"]->reshape(
        {batch_size, out_feats, in_feats, filter_h, filter_w});
    ops::sum(filter_grad, cache["filter_grad_unfolded"].get(), 0);

    // K^T
    // Reshape K to (o_f, i_f * k_h * k_w)
    filter->reshape({out_feats, in_feats * filter_h * filter_w});
    utils::set_array_cache(cache, "filter_t",
                           {in_feats * filter_h * filter_w, out_feats});
    ops::transpose(cache["filter_t"].get(), filter);

    // dL/dCols
    utils::set_array_cache(
        cache, "col_grad",
        {batch_size, in_feats * filter_h * filter_w, out_h * out_w});
    ops::matmul(cache["col_grad"].get(), cache["filter_t"].get(), output_grad,
                1);

    // dL/dX
    col2im(cache["col_grad"].get(), input_grad, pad_h, pad_w, filter_h,
           filter_w, stride_h, stride_w);

    // Restore shape
    output_grad->reshape({batch_size, out_feats, out_h, out_w});
    filter->reshape({out_feats, in_feats, filter_h, filter_w});
}

void conv_backward_bias(Array *bias_grad, const Array *output_grad,
                        ArrayMap &cache) {
    const ShapeType &bias_grad_shape = bias_grad->get_shape();
    const ShapeType &output_grad_shape = output_grad->get_shape();

    CHECK_EQ(bias_grad_shape[0], 1,
             "conv_backward_bias: bias grad isn't a column vector");

    int batch_size = output_grad_shape[0];
    int out_feats = output_grad_shape[1];
    int out_h = output_grad_shape[2];

    CHECK_EQ(bias_grad_shape[1], out_feats,
             "conv_backward_bias: mismatch between bias grad size and number "
             "of output features");

    utils::set_array_cache(cache, "fold3", {batch_size, out_feats, out_h});
    utils::set_array_cache(cache, "fold2", {batch_size, out_feats});

    ops::sum(cache["fold3"].get(), output_grad, 3);
    ops::sum(cache["fold2"].get(), cache["fold3"].get(), 2);
    ops::sum(bias_grad, cache["fold2"].get(), 0, false);
}

Conv2D::Conv2D(int in_feats, int out_feats, int in_h, int in_w, int pad_h,
               int pad_w, int filter_h, int filter_w, int stride_h,
               int stride_w, const Initializer *init)
    : in_feats(in_feats), out_feats(out_feats), in_h(in_h), in_w(in_w),
      pad_h(pad_h), pad_w(pad_w), filter_h(filter_h), filter_w(filter_w),
      stride_h(stride_h), stride_w(stride_w) {
    filter.reset(new Array({out_feats, in_feats, filter_h, filter_w}));
    filter_grad.reset(new Array({out_feats, in_feats, filter_h, filter_w}));
    bias.reset(new Array({1, out_feats}));
    bias_grad.reset(new Array({1, out_feats}));

    // Initialize parameters
    int fan_in = filter_h * filter_w * in_feats;
    int fan_out = filter_h * filter_w * out_feats;
    init->initialize(filter.get(), fan_in, fan_out);
    init->initialize(bias.get(), fan_in, fan_out);
}

std::vector<Param> Conv2D::get_parameters() {
    return {std::make_pair(filter.get(), filter_grad.get()),
            std::make_pair(bias.get(), bias_grad.get())};
}

void Conv2D::forward() {
    const Array *input = prev->get_output();

    int batch_size = input->get_shape()[0];
    int out_h = (in_h + 2 * pad_h - filter_h) / stride_h + 1;
    int out_w = (in_w + 2 * pad_w - filter_w) / stride_w + 1;

    utils::set_array_ptr(output, {batch_size, out_feats, out_h, out_w});
    utils::set_array_ptr(
        col, {batch_size, in_feats * filter_h * filter_w, out_h * out_w});

    conv_forward(output.get(), input, col.get(), filter.get(), pad_h, pad_w,
                 stride_h, stride_w);
    conv_forward_bias(output.get(), bias.get());
}

void Conv2D::backward() {
    const Array *input = prev->get_output();
    Array *output_grad = next->get_grad();

    utils::set_array_ptr(grad, input->get_shape());

    conv_backward_bias(bias_grad.get(), output_grad, cache);
    conv_backward(grad.get(), filter_grad.get(), output_grad, input,
                  filter.get(), col.get(), pad_h, pad_w, stride_h, stride_w,
                  cache);
}

} // namespace nnv2